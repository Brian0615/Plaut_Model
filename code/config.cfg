[setup]
# total_epochs: total number of training epochs
# anchor_epoch: number of epochs until adding anchors
# inital_epochs: number of epochs before switching optimizers

total_epochs = 600
anchor_epoch = 500
inital_epochs = 10

[initial]
# optim: optimizer (Adam or SGD)
# lr: learning rate
# momentum: momentum (does not matter if Adam is used)
# wd: weight decay

optim = SGD
lr = 0.1
momentum = 0.9
wd = 0.0001

[final]
# optim: optimizer (Adam or SGD)
# lr: learning rate
# momentum: momentum (does not matter if Adam is used)
# wd: weight decay

optim = Adam
lr = 0.1
momentum = 0.9
wd = 0.0001
