[setup]
# total_epochs: total number of training epochs
# anchor_epoch: number of epochs until adding anchors
# save_freq: frequency for saving loss or accuracy plots
# print_freq: frequency for printing statistics

random_seed = 1
total_epochs = 1000
anchor_epoch = 500
plot_freq = 50
print_freq = 1
save_freq = 1
checkpoint_epochs = 500
prev_checkpoint = ../results/feb04_test01/checkpoint_500.tar

[dataset]
# plaut: filepath for plaut dataset file
# anchor: filepath for anchor dataset file
# probe: filepath for probe dataset file

# List of useful filepaths:
# original plaut: ../dataset/plaut_dataset.csv
# collapsed plaut: ../dataset/plaut_dataset_collapsed.csv
# old anchors and probes: ../dataset/anchors.csv, ../dataset/probes.csv
# new anchors: (N) ../dataset/anchors_new1.csv, (N/2)../dataset/anchors_new2.csv, (N/3)../dataset/anchors_new3.csv
# new probes: ../dataset/probes_new.csv


plaut = ../dataset/plaut_dataset_collapsed.csv
anchor = ../dataset/csv_by_word/plone.csv
probe = ../dataset/probes_new.csv

# DEFINITION OF DIFFERENT OPTIMIZER SETTINGS
#  > start: epoch to switch to new optimizer
#  > optim: optimizer (ONLY Adam or SGD)
#  > lr: learning rate (0.001 used in paper)
#  > momentum: momentum (0 initially, then 0.9 used in paper)
#    NOTE: a momentum value MUST be included (it will simply be ignored for Adam)
#  > wd: weight decay (0.0001 used in paper)
# NOTE: use a new [partX] for each time the optimizer needs to be changed

[part1]
start = 0
optim = SGD
lr = 0.001
momentum = 0
wd = 0.000001

[part2]
start = 10
optim = Adam
lr = 0.01
momentum = 0
wd = 0.000001


